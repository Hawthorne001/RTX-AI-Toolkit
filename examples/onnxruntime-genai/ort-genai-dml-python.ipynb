{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfcd2651-4870-4265-8e06-7db587630edc",
   "metadata": {},
   "source": [
    "## Installing ONNXRuntime-GenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d712f8fd-7210-4041-86e6-3d27f3da9eb0",
   "metadata": {},
   "source": [
    "Installing the correct package of onnxruntime-genai is important, as the suffix of the package shows which Execution Provider is included with the underlying ONNXRuntime framework. Here, we install the `onnxruntime-genai-directml` package so we can execute models through the DirectML Execution Provider.\n",
    "\n",
    "**Note: The DirectML Execution Provider stems from DirectX, and thus is only available on Windows systems.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24648873-cd65-4dc8-8bf9-fbb7c1e62a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install onnx==1.16.1\n",
    "!pip install transformers torch numpy\n",
    "!pip install onnxruntime-genai-directml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e5a302-9838-4329-b964-b56f668bd064",
   "metadata": {},
   "source": [
    "## Getting a compatible ONNX model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f180dab-52b1-4e85-b26e-cd22aa3a8ceb",
   "metadata": {},
   "source": [
    "Because ONNXRuntime-GenAI is specialized for generative ONNX models, it only supports models within this class and not models that rely on a single inference, such as classifier models. There are a couple ways to obtain an ONNX model that can be used with ONNXRuntime-GenAI - we're going to use the model builder, a tool included in the ORT-GenAI package, to get [Microsoft's Phi-3.5 model](https://huggingface.co/microsoft/Phi-3.5-mini-instruct/tree/main)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3bb695f-3aba-409e-86e3-78b05b88e43b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid precision + execution provider combinations are: FP32 CPU, FP32 CUDA, FP16 CUDA, FP16 DML, INT4 CPU, INT4 CUDA, INT4 DML\n",
      "Extra options: {}\n",
      "GroupQueryAttention (GQA) is used in this model.\n",
      "Reading embedding layer\n",
      "Reading decoder layer 0\n",
      "Reading decoder layer 1\n",
      "Reading decoder layer 2\n",
      "Reading decoder layer 3\n",
      "Reading decoder layer 4\n",
      "Reading decoder layer 5\n",
      "Reading decoder layer 6\n",
      "Reading decoder layer 7\n",
      "Reading decoder layer 8\n",
      "Reading decoder layer 9\n",
      "Reading decoder layer 10\n",
      "Reading decoder layer 11\n",
      "Reading decoder layer 12\n",
      "Reading decoder layer 13\n",
      "Reading decoder layer 14\n",
      "Reading decoder layer 15\n",
      "Reading decoder layer 16\n",
      "Reading decoder layer 17\n",
      "Reading decoder layer 18\n",
      "Reading decoder layer 19\n",
      "Reading decoder layer 20\n",
      "Reading decoder layer 21\n",
      "Reading decoder layer 22\n",
      "Reading decoder layer 23\n",
      "Reading decoder layer 24\n",
      "Reading decoder layer 25\n",
      "Reading decoder layer 26\n",
      "Reading decoder layer 27\n",
      "Reading decoder layer 28\n",
      "Reading decoder layer 29\n",
      "Reading decoder layer 30\n",
      "Reading decoder layer 31\n",
      "Reading final norm\n",
      "Reading LM head\n",
      "Saving ONNX model in phi-3-dml\n",
      "Saving GenAI config in phi-3-dml\n",
      "Saving processing files in phi-3-dml for GenAI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NVIDIA\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:961: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\NVIDIA\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\NVIDIA\\ort-genai\\cache_dir\\models--microsoft--Phi-3.5-mini-instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3.5-mini-instruct:\n",
      "- configuration_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "C:\\Users\\NVIDIA\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:469: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3.5-mini-instruct:\n",
      "- modeling_phi3.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "2024-09-25 13:00:13,946 transformers_modules.microsoft.Phi-3.5-mini-instruct.af0dfb8029e8a74545d0736d30cb6b58d2f0f3f0.modeling_phi3 [WARNING] - `flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "2024-09-25 13:00:13,946 transformers_modules.microsoft.Phi-3.5-mini-instruct.af0dfb8029e8a74545d0736d30cb6b58d2f0f3f0.modeling_phi3 [WARNING] - Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "\n",
      "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Downloading shards:  50%|#####     | 1/2 [00:47<00:47, 47.16s/it]\n",
      "Downloading shards: 100%|##########| 2/2 [01:19<00:00, 38.24s/it]\n",
      "Downloading shards: 100%|##########| 2/2 [01:19<00:00, 39.58s/it]\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:  50%|#####     | 1/2 [00:02<00:02,  2.76s/it]\n",
      "Loading checkpoint shards: 100%|##########| 2/2 [00:04<00:00,  1.96s/it]\n",
      "Loading checkpoint shards: 100%|##########| 2/2 [00:04<00:00,  2.08s/it]\n",
      "2024-09-25 13:03:02,860 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.0/attn/q_proj/MatMul ...\n",
      "2024-09-25 13:03:02,890 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.0/attn/q_proj/MatMul ...\n",
      "2024-09-25 13:03:02,890 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.0/attn/k_proj/MatMul ...\n",
      "2024-09-25 13:03:02,905 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.0/attn/k_proj/MatMul ...\n",
      "2024-09-25 13:03:02,905 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.0/attn/v_proj/MatMul ...\n",
      "2024-09-25 13:03:02,921 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.0/attn/v_proj/MatMul ...\n",
      "2024-09-25 13:03:02,921 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.0/attn/o_proj/MatMul ...\n",
      "2024-09-25 13:03:02,932 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.0/attn/o_proj/MatMul ...\n",
      "2024-09-25 13:03:02,932 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.0/mlp/gate_proj/MatMul ...\n",
      "2024-09-25 13:03:02,973 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.0/mlp/gate_proj/MatMul ...\n",
      "2024-09-25 13:03:02,973 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.0/mlp/up_proj/MatMul ...\n",
      "2024-09-25 13:03:03,003 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.0/mlp/up_proj/MatMul ...\n",
      "2024-09-25 13:03:03,003 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.0/mlp/down_proj/MatMul ...\n",
      "2024-09-25 13:03:03,044 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.0/mlp/down_proj/MatMul ...\n",
      "2024-09-25 13:03:03,044 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.1/attn/q_proj/MatMul ...\n",
      "2024-09-25 13:03:03,057 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.1/attn/q_proj/MatMul ...\n",
      "2024-09-25 13:03:03,060 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.1/attn/k_proj/MatMul ...\n",
      "2024-09-25 13:03:03,065 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.1/attn/k_proj/MatMul ...\n",
      "2024-09-25 13:03:03,075 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.1/attn/v_proj/MatMul ...\n",
      "2024-09-25 13:03:03,088 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.1/attn/v_proj/MatMul ...\n",
      "2024-09-25 13:03:03,088 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.1/attn/o_proj/MatMul ...\n",
      "2024-09-25 13:03:03,095 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.1/attn/o_proj/MatMul ...\n",
      "2024-09-25 13:03:03,095 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.1/mlp/gate_proj/MatMul ...\n",
      "2024-09-25 13:03:03,146 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.1/mlp/gate_proj/MatMul ...\n",
      "2024-09-25 13:03:03,146 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.1/mlp/up_proj/MatMul ...\n",
      "2024-09-25 13:03:03,188 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.1/mlp/up_proj/MatMul ...\n",
      "2024-09-25 13:03:03,188 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.1/mlp/down_proj/MatMul ...\n",
      "2024-09-25 13:03:03,220 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.1/mlp/down_proj/MatMul ...\n",
      "2024-09-25 13:03:03,220 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.2/attn/q_proj/MatMul ...\n",
      "2024-09-25 13:03:03,237 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.2/attn/q_proj/MatMul ...\n",
      "2024-09-25 13:03:03,237 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.2/attn/k_proj/MatMul ...\n",
      "2024-09-25 13:03:03,252 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.2/attn/k_proj/MatMul ...\n",
      "2024-09-25 13:03:03,252 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.2/attn/v_proj/MatMul ...\n",
      "2024-09-25 13:03:03,268 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.2/attn/v_proj/MatMul ...\n",
      "2024-09-25 13:03:03,268 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.2/attn/o_proj/MatMul ...\n",
      "2024-09-25 13:03:03,281 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.2/attn/o_proj/MatMul ...\n",
      "2024-09-25 13:03:03,281 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.2/mlp/gate_proj/MatMul ...\n",
      "2024-09-25 13:03:03,316 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.2/mlp/gate_proj/MatMul ...\n",
      "2024-09-25 13:03:03,316 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.2/mlp/up_proj/MatMul ...\n",
      "2024-09-25 13:03:03,351 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.2/mlp/up_proj/MatMul ...\n",
      "2024-09-25 13:03:03,351 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.2/mlp/down_proj/MatMul ...\n",
      "2024-09-25 13:03:03,381 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.2/mlp/down_proj/MatMul ...\n",
      "2024-09-25 13:03:03,381 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.3/attn/q_proj/MatMul ...\n",
      "2024-09-25 13:03:03,400 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.3/attn/q_proj/MatMul ...\n",
      "2024-09-25 13:03:03,400 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.3/attn/k_proj/MatMul ...\n",
      "2024-09-25 13:03:03,412 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.3/attn/k_proj/MatMul ...\n",
      "2024-09-25 13:03:03,412 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.3/attn/v_proj/MatMul ...\n",
      "2024-09-25 13:03:03,428 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.3/attn/v_proj/MatMul ...\n",
      "2024-09-25 13:03:03,428 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.3/attn/o_proj/MatMul ...\n",
      "2024-09-25 13:03:03,444 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.3/attn/o_proj/MatMul ...\n",
      "2024-09-25 13:03:03,444 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.3/mlp/gate_proj/MatMul ...\n",
      "2024-09-25 13:03:03,476 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.3/mlp/gate_proj/MatMul ...\n",
      "2024-09-25 13:03:03,476 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.3/mlp/up_proj/MatMul ...\n",
      "2024-09-25 13:03:03,513 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.3/mlp/up_proj/MatMul ...\n",
      "2024-09-25 13:03:03,513 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.3/mlp/down_proj/MatMul ...\n",
      "2024-09-25 13:03:03,544 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.3/mlp/down_proj/MatMul ...\n",
      "2024-09-25 13:03:03,544 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.4/attn/q_proj/MatMul ...\n",
      "2024-09-25 13:03:03,564 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.4/attn/q_proj/MatMul ...\n",
      "2024-09-25 13:03:03,564 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.4/attn/k_proj/MatMul ...\n",
      "2024-09-25 13:03:03,574 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.4/attn/k_proj/MatMul ...\n",
      "2024-09-25 13:03:03,574 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.4/attn/v_proj/MatMul ...\n",
      "2024-09-25 13:03:03,588 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.4/attn/v_proj/MatMul ...\n",
      "2024-09-25 13:03:03,588 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.4/attn/o_proj/MatMul ...\n",
      "2024-09-25 13:03:03,604 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.4/attn/o_proj/MatMul ...\n",
      "2024-09-25 13:03:03,604 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.4/mlp/gate_proj/MatMul ...\n",
      "2024-09-25 13:03:03,636 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.4/mlp/gate_proj/MatMul ...\n",
      "2024-09-25 13:03:03,636 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.4/mlp/up_proj/MatMul ...\n",
      "2024-09-25 13:03:03,675 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.4/mlp/up_proj/MatMul ...\n",
      "2024-09-25 13:03:03,675 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.4/mlp/down_proj/MatMul ...\n",
      "2024-09-25 13:03:03,705 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.4/mlp/down_proj/MatMul ...\n",
      "2024-09-25 13:03:03,705 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.5/attn/q_proj/MatMul ...\n",
      "2024-09-25 13:03:03,716 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.5/attn/q_proj/MatMul ...\n",
      "2024-09-25 13:03:03,716 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.5/attn/k_proj/MatMul ...\n",
      "2024-09-25 13:03:03,732 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.5/attn/k_proj/MatMul ...\n",
      "2024-09-25 13:03:03,736 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.5/attn/v_proj/MatMul ...\n",
      "2024-09-25 13:03:03,748 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.5/attn/v_proj/MatMul ...\n",
      "2024-09-25 13:03:03,748 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.5/attn/o_proj/MatMul ...\n",
      "2024-09-25 13:03:03,757 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.5/attn/o_proj/MatMul ...\n",
      "2024-09-25 13:03:03,757 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.5/mlp/gate_proj/MatMul ...\n",
      "2024-09-25 13:03:03,797 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.5/mlp/gate_proj/MatMul ...\n",
      "2024-09-25 13:03:03,797 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.5/mlp/up_proj/MatMul ...\n",
      "2024-09-25 13:03:03,829 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.5/mlp/up_proj/MatMul ...\n",
      "2024-09-25 13:03:03,829 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.5/mlp/down_proj/MatMul ...\n",
      "2024-09-25 13:03:03,867 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.5/mlp/down_proj/MatMul ...\n",
      "2024-09-25 13:03:03,867 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.6/attn/q_proj/MatMul ...\n",
      "2024-09-25 13:03:03,877 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.6/attn/q_proj/MatMul ...\n",
      "2024-09-25 13:03:03,877 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.6/attn/k_proj/MatMul ...\n",
      "2024-09-25 13:03:03,898 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.6/attn/k_proj/MatMul ...\n",
      "2024-09-25 13:03:03,898 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.6/attn/v_proj/MatMul ...\n",
      "2024-09-25 13:03:03,911 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.6/attn/v_proj/MatMul ...\n",
      "2024-09-25 13:03:03,912 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.6/attn/o_proj/MatMul ...\n",
      "2024-09-25 13:03:03,925 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.6/attn/o_proj/MatMul ...\n",
      "2024-09-25 13:03:03,925 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.6/mlp/gate_proj/MatMul ...\n",
      "2024-09-25 13:03:03,960 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.6/mlp/gate_proj/MatMul ...\n",
      "2024-09-25 13:03:03,961 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.6/mlp/up_proj/MatMul ...\n",
      "2024-09-25 13:03:03,994 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.6/mlp/up_proj/MatMul ...\n",
      "2024-09-25 13:03:03,994 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.6/mlp/down_proj/MatMul ...\n",
      "2024-09-25 13:03:04,025 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.6/mlp/down_proj/MatMul ...\n",
      "2024-09-25 13:03:04,025 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.7/attn/q_proj/MatMul ...\n",
      "2024-09-25 13:03:04,041 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.7/attn/q_proj/MatMul ...\n",
      "2024-09-25 13:03:04,041 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.7/attn/k_proj/MatMul ...\n",
      "2024-09-25 13:03:04,056 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.7/attn/k_proj/MatMul ...\n",
      "2024-09-25 13:03:04,056 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.7/attn/v_proj/MatMul ...\n",
      "2024-09-25 13:03:04,065 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.7/attn/v_proj/MatMul ...\n",
      "2024-09-25 13:03:04,065 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.7/attn/o_proj/MatMul ...\n",
      "2024-09-25 13:03:04,075 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.7/attn/o_proj/MatMul ...\n",
      "2024-09-25 13:03:04,075 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.7/mlp/gate_proj/MatMul ...\n",
      "2024-09-25 13:03:04,116 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.7/mlp/gate_proj/MatMul ...\n",
      "2024-09-25 13:03:04,116 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.7/mlp/up_proj/MatMul ...\n",
      "2024-09-25 13:03:04,154 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.7/mlp/up_proj/MatMul ...\n",
      "2024-09-25 13:03:04,154 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.7/mlp/down_proj/MatMul ...\n",
      "2024-09-25 13:03:04,188 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.7/mlp/down_proj/MatMul ...\n",
      "2024-09-25 13:03:04,188 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.8/attn/q_proj/MatMul ...\n",
      "2024-09-25 13:03:04,202 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.8/attn/q_proj/MatMul ...\n",
      "2024-09-25 13:03:04,202 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.8/attn/k_proj/MatMul ...\n",
      "2024-09-25 13:03:04,213 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.8/attn/k_proj/MatMul ...\n",
      "2024-09-25 13:03:04,213 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.8/attn/v_proj/MatMul ...\n",
      "2024-09-25 13:03:04,229 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.8/attn/v_proj/MatMul ...\n",
      "2024-09-25 13:03:04,229 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.8/attn/o_proj/MatMul ...\n",
      "2024-09-25 13:03:04,244 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.8/attn/o_proj/MatMul ...\n",
      "2024-09-25 13:03:04,244 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.8/mlp/gate_proj/MatMul ...\n",
      "2024-09-25 13:03:04,281 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.8/mlp/gate_proj/MatMul ...\n",
      "2024-09-25 13:03:04,281 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.8/mlp/up_proj/MatMul ...\n",
      "2024-09-25 13:03:04,313 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.8/mlp/up_proj/MatMul ...\n",
      "2024-09-25 13:03:04,313 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.8/mlp/down_proj/MatMul ...\n",
      "2024-09-25 13:03:04,345 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.8/mlp/down_proj/MatMul ...\n",
      "2024-09-25 13:03:04,345 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.9/attn/q_proj/MatMul ...\n",
      "2024-09-25 13:03:04,361 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.9/attn/q_proj/MatMul ...\n",
      "2024-09-25 13:03:04,361 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.9/attn/k_proj/MatMul ...\n",
      "2024-09-25 13:03:04,377 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.9/attn/k_proj/MatMul ...\n",
      "2024-09-25 13:03:04,377 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.9/attn/v_proj/MatMul ...\n",
      "2024-09-25 13:03:04,381 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.9/attn/v_proj/MatMul ...\n",
      "2024-09-25 13:03:04,381 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.9/attn/o_proj/MatMul ...\n",
      "2024-09-25 13:03:04,401 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.9/attn/o_proj/MatMul ...\n",
      "2024-09-25 13:03:04,401 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.9/mlp/gate_proj/MatMul ...\n",
      "2024-09-25 13:03:04,432 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.9/mlp/gate_proj/MatMul ...\n",
      "2024-09-25 13:03:04,432 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.9/mlp/up_proj/MatMul ...\n",
      "2024-09-25 13:03:04,473 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.9/mlp/up_proj/MatMul ...\n",
      "2024-09-25 13:03:04,473 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.9/mlp/down_proj/MatMul ...\n",
      "2024-09-25 13:03:04,505 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.9/mlp/down_proj/MatMul ...\n",
      "2024-09-25 13:03:04,505 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.10/attn/q_proj/MatMul ...\n",
      "2024-09-25 13:03:04,525 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.10/attn/q_proj/MatMul ...\n",
      "2024-09-25 13:03:04,525 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.10/attn/k_proj/MatMul ...\n",
      "2024-09-25 13:03:04,537 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.10/attn/k_proj/MatMul ...\n",
      "2024-09-25 13:03:04,537 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.10/attn/v_proj/MatMul ...\n",
      "2024-09-25 13:03:04,554 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.10/attn/v_proj/MatMul ...\n",
      "2024-09-25 13:03:04,554 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.10/attn/o_proj/MatMul ...\n",
      "2024-09-25 13:03:04,564 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.10/attn/o_proj/MatMul ...\n",
      "2024-09-25 13:03:04,564 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.10/mlp/gate_proj/MatMul ...\n",
      "2024-09-25 13:03:04,601 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.10/mlp/gate_proj/MatMul ...\n",
      "2024-09-25 13:03:04,601 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.10/mlp/up_proj/MatMul ...\n",
      "2024-09-25 13:03:04,635 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.10/mlp/up_proj/MatMul ...\n",
      "2024-09-25 13:03:04,635 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.10/mlp/down_proj/MatMul ...\n",
      "2024-09-25 13:03:04,665 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.10/mlp/down_proj/MatMul ...\n",
      "2024-09-25 13:03:04,665 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.11/attn/q_proj/MatMul ...\n",
      "2024-09-25 13:03:04,685 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.11/attn/q_proj/MatMul ...\n",
      "2024-09-25 13:03:04,685 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.11/attn/k_proj/MatMul ...\n",
      "2024-09-25 13:03:04,697 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.11/attn/k_proj/MatMul ...\n",
      "2024-09-25 13:03:04,697 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.11/attn/v_proj/MatMul ...\n",
      "2024-09-25 13:03:04,716 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.11/attn/v_proj/MatMul ...\n",
      "2024-09-25 13:03:04,716 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.11/attn/o_proj/MatMul ...\n",
      "2024-09-25 13:03:04,729 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.11/attn/o_proj/MatMul ...\n",
      "2024-09-25 13:03:04,729 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.11/mlp/gate_proj/MatMul ...\n",
      "2024-09-25 13:03:04,761 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.11/mlp/gate_proj/MatMul ...\n",
      "2024-09-25 13:03:04,761 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.11/mlp/up_proj/MatMul ...\n",
      "2024-09-25 13:03:04,797 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.11/mlp/up_proj/MatMul ...\n",
      "2024-09-25 13:03:04,797 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.11/mlp/down_proj/MatMul ...\n",
      "2024-09-25 13:03:04,827 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.11/mlp/down_proj/MatMul ...\n",
      "2024-09-25 13:03:04,837 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.12/attn/q_proj/MatMul ...\n",
      "2024-09-25 13:03:04,847 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.12/attn/q_proj/MatMul ...\n",
      "2024-09-25 13:03:04,847 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.12/attn/k_proj/MatMul ...\n",
      "2024-09-25 13:03:04,857 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.12/attn/k_proj/MatMul ...\n",
      "2024-09-25 13:03:04,857 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.12/attn/v_proj/MatMul ...\n",
      "2024-09-25 13:03:04,873 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.12/attn/v_proj/MatMul ...\n",
      "2024-09-25 13:03:04,877 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.12/attn/o_proj/MatMul ...\n",
      "2024-09-25 13:03:04,889 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.12/attn/o_proj/MatMul ...\n",
      "2024-09-25 13:03:04,889 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.12/mlp/gate_proj/MatMul ...\n",
      "2024-09-25 13:03:04,925 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.12/mlp/gate_proj/MatMul ...\n",
      "2024-09-25 13:03:04,926 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.12/mlp/up_proj/MatMul ...\n",
      "2024-09-25 13:03:04,961 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.12/mlp/up_proj/MatMul ...\n",
      "2024-09-25 13:03:04,961 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.12/mlp/down_proj/MatMul ...\n",
      "2024-09-25 13:03:04,996 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.12/mlp/down_proj/MatMul ...\n",
      "2024-09-25 13:03:04,996 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.13/attn/q_proj/MatMul ...\n",
      "2024-09-25 13:03:05,011 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.13/attn/q_proj/MatMul ...\n",
      "2024-09-25 13:03:05,011 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.13/attn/k_proj/MatMul ...\n",
      "2024-09-25 13:03:05,026 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.13/attn/k_proj/MatMul ...\n",
      "2024-09-25 13:03:05,026 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.13/attn/v_proj/MatMul ...\n",
      "2024-09-25 13:03:05,039 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.13/attn/v_proj/MatMul ...\n",
      "2024-09-25 13:03:05,039 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.13/attn/o_proj/MatMul ...\n",
      "2024-09-25 13:03:05,049 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.13/attn/o_proj/MatMul ...\n",
      "2024-09-25 13:03:05,049 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.13/mlp/gate_proj/MatMul ...\n",
      "2024-09-25 13:03:05,088 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.13/mlp/gate_proj/MatMul ...\n",
      "2024-09-25 13:03:05,088 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.13/mlp/up_proj/MatMul ...\n",
      "2024-09-25 13:03:05,124 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.13/mlp/up_proj/MatMul ...\n",
      "2024-09-25 13:03:05,124 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.13/mlp/down_proj/MatMul ...\n",
      "2024-09-25 13:03:05,158 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.13/mlp/down_proj/MatMul ...\n",
      "2024-09-25 13:03:05,159 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.14/attn/q_proj/MatMul ...\n",
      "2024-09-25 13:03:05,172 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.14/attn/q_proj/MatMul ...\n",
      "2024-09-25 13:03:05,172 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.14/attn/k_proj/MatMul ...\n",
      "2024-09-25 13:03:05,183 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.14/attn/k_proj/MatMul ...\n",
      "2024-09-25 13:03:05,183 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.14/attn/v_proj/MatMul ...\n",
      "2024-09-25 13:03:05,199 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.14/attn/v_proj/MatMul ...\n",
      "2024-09-25 13:03:05,200 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.14/attn/o_proj/MatMul ...\n",
      "2024-09-25 13:03:05,213 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.14/attn/o_proj/MatMul ...\n",
      "2024-09-25 13:03:05,213 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.14/mlp/gate_proj/MatMul ...\n",
      "2024-09-25 13:03:05,244 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.14/mlp/gate_proj/MatMul ...\n",
      "2024-09-25 13:03:05,244 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.14/mlp/up_proj/MatMul ...\n",
      "2024-09-25 13:03:05,285 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.14/mlp/up_proj/MatMul ...\n",
      "2024-09-25 13:03:05,285 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.14/mlp/down_proj/MatMul ...\n",
      "2024-09-25 13:03:05,320 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.14/mlp/down_proj/MatMul ...\n",
      "2024-09-25 13:03:05,320 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.15/attn/q_proj/MatMul ...\n",
      "2024-09-25 13:03:05,334 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.15/attn/q_proj/MatMul ...\n",
      "2024-09-25 13:03:05,334 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.15/attn/k_proj/MatMul ...\n",
      "2024-09-25 13:03:05,346 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.15/attn/k_proj/MatMul ...\n",
      "2024-09-25 13:03:05,346 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.15/attn/v_proj/MatMul ...\n",
      "2024-09-25 13:03:05,356 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.15/attn/v_proj/MatMul ...\n",
      "2024-09-25 13:03:05,356 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.15/attn/o_proj/MatMul ...\n",
      "2024-09-25 13:03:05,371 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.15/attn/o_proj/MatMul ...\n",
      "2024-09-25 13:03:05,371 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.15/mlp/gate_proj/MatMul ...\n",
      "2024-09-25 13:03:05,406 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.15/mlp/gate_proj/MatMul ...\n",
      "2024-09-25 13:03:05,406 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.15/mlp/up_proj/MatMul ...\n",
      "2024-09-25 13:03:05,447 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.15/mlp/up_proj/MatMul ...\n",
      "2024-09-25 13:03:05,448 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.15/mlp/down_proj/MatMul ...\n",
      "2024-09-25 13:03:05,482 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.15/mlp/down_proj/MatMul ...\n",
      "2024-09-25 13:03:05,482 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.16/attn/q_proj/MatMul ...\n",
      "2024-09-25 13:03:05,495 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.16/attn/q_proj/MatMul ...\n",
      "2024-09-25 13:03:05,495 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.16/attn/k_proj/MatMul ...\n",
      "2024-09-25 13:03:05,509 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.16/attn/k_proj/MatMul ...\n",
      "2024-09-25 13:03:05,509 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.16/attn/v_proj/MatMul ...\n",
      "2024-09-25 13:03:05,522 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.16/attn/v_proj/MatMul ...\n",
      "2024-09-25 13:03:05,522 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.16/attn/o_proj/MatMul ...\n",
      "2024-09-25 13:03:05,537 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.16/attn/o_proj/MatMul ...\n",
      "2024-09-25 13:03:05,537 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.16/mlp/gate_proj/MatMul ...\n",
      "2024-09-25 13:03:05,572 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.16/mlp/gate_proj/MatMul ...\n",
      "2024-09-25 13:03:05,572 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.16/mlp/up_proj/MatMul ...\n",
      "2024-09-25 13:03:05,608 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.16/mlp/up_proj/MatMul ...\n",
      "2024-09-25 13:03:05,609 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.16/mlp/down_proj/MatMul ...\n",
      "2024-09-25 13:03:05,644 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.16/mlp/down_proj/MatMul ...\n",
      "2024-09-25 13:03:05,644 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.17/attn/q_proj/MatMul ...\n",
      "2024-09-25 13:03:05,658 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.17/attn/q_proj/MatMul ...\n",
      "2024-09-25 13:03:05,659 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.17/attn/k_proj/MatMul ...\n",
      "2024-09-25 13:03:05,671 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.17/attn/k_proj/MatMul ...\n",
      "2024-09-25 13:03:05,671 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.17/attn/v_proj/MatMul ...\n",
      "2024-09-25 13:03:05,685 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.17/attn/v_proj/MatMul ...\n",
      "2024-09-25 13:03:05,685 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.17/attn/o_proj/MatMul ...\n",
      "2024-09-25 13:03:05,698 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.17/attn/o_proj/MatMul ...\n",
      "2024-09-25 13:03:05,698 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.17/mlp/gate_proj/MatMul ...\n",
      "2024-09-25 13:03:05,734 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.17/mlp/gate_proj/MatMul ...\n",
      "2024-09-25 13:03:05,734 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.17/mlp/up_proj/MatMul ...\n",
      "2024-09-25 13:03:05,770 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.17/mlp/up_proj/MatMul ...\n",
      "2024-09-25 13:03:05,770 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.17/mlp/down_proj/MatMul ...\n",
      "2024-09-25 13:03:05,800 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.17/mlp/down_proj/MatMul ...\n",
      "2024-09-25 13:03:05,800 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.18/attn/q_proj/MatMul ...\n",
      "2024-09-25 13:03:05,811 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.18/attn/q_proj/MatMul ...\n",
      "2024-09-25 13:03:05,811 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.18/attn/k_proj/MatMul ...\n",
      "2024-09-25 13:03:05,824 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.18/attn/k_proj/MatMul ...\n",
      "2024-09-25 13:03:05,824 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.18/attn/v_proj/MatMul ...\n",
      "2024-09-25 13:03:05,844 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.18/attn/v_proj/MatMul ...\n",
      "2024-09-25 13:03:05,844 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.18/attn/o_proj/MatMul ...\n",
      "2024-09-25 13:03:05,856 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.18/attn/o_proj/MatMul ...\n",
      "2024-09-25 13:03:05,856 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.18/mlp/gate_proj/MatMul ...\n",
      "2024-09-25 13:03:05,888 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.18/mlp/gate_proj/MatMul ...\n",
      "2024-09-25 13:03:05,888 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.18/mlp/up_proj/MatMul ...\n",
      "2024-09-25 13:03:05,929 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.18/mlp/up_proj/MatMul ...\n",
      "2024-09-25 13:03:05,930 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.18/mlp/down_proj/MatMul ...\n",
      "2024-09-25 13:03:05,963 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.18/mlp/down_proj/MatMul ...\n",
      "2024-09-25 13:03:05,963 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.19/attn/q_proj/MatMul ...\n",
      "2024-09-25 13:03:05,978 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.19/attn/q_proj/MatMul ...\n",
      "2024-09-25 13:03:05,979 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.19/attn/k_proj/MatMul ...\n",
      "2024-09-25 13:03:05,992 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.19/attn/k_proj/MatMul ...\n",
      "2024-09-25 13:03:05,992 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.19/attn/v_proj/MatMul ...\n",
      "2024-09-25 13:03:06,002 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.19/attn/v_proj/MatMul ...\n",
      "2024-09-25 13:03:06,002 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.19/attn/o_proj/MatMul ...\n",
      "2024-09-25 13:03:06,012 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.19/attn/o_proj/MatMul ...\n",
      "2024-09-25 13:03:06,012 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.19/mlp/gate_proj/MatMul ...\n",
      "2024-09-25 13:03:06,053 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.19/mlp/gate_proj/MatMul ...\n",
      "2024-09-25 13:03:06,053 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.19/mlp/up_proj/MatMul ...\n",
      "2024-09-25 13:03:06,091 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.19/mlp/up_proj/MatMul ...\n",
      "2024-09-25 13:03:06,091 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.19/mlp/down_proj/MatMul ...\n",
      "2024-09-25 13:03:06,123 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.19/mlp/down_proj/MatMul ...\n",
      "2024-09-25 13:03:06,123 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.20/attn/q_proj/MatMul ...\n",
      "2024-09-25 13:03:06,141 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.20/attn/q_proj/MatMul ...\n",
      "2024-09-25 13:03:06,141 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.20/attn/k_proj/MatMul ...\n",
      "2024-09-25 13:03:06,154 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.20/attn/k_proj/MatMul ...\n",
      "2024-09-25 13:03:06,154 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.20/attn/v_proj/MatMul ...\n",
      "2024-09-25 13:03:06,168 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.20/attn/v_proj/MatMul ...\n",
      "2024-09-25 13:03:06,168 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.20/attn/o_proj/MatMul ...\n",
      "2024-09-25 13:03:06,183 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.20/attn/o_proj/MatMul ...\n",
      "2024-09-25 13:03:06,184 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.20/mlp/gate_proj/MatMul ...\n",
      "2024-09-25 13:03:06,215 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.20/mlp/gate_proj/MatMul ...\n",
      "2024-09-25 13:03:06,215 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.20/mlp/up_proj/MatMul ...\n",
      "2024-09-25 13:03:06,255 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.20/mlp/up_proj/MatMul ...\n",
      "2024-09-25 13:03:06,255 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.20/mlp/down_proj/MatMul ...\n",
      "2024-09-25 13:03:06,293 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.20/mlp/down_proj/MatMul ...\n",
      "2024-09-25 13:03:06,294 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.21/attn/q_proj/MatMul ...\n",
      "2024-09-25 13:03:06,308 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.21/attn/q_proj/MatMul ...\n",
      "2024-09-25 13:03:06,308 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.21/attn/k_proj/MatMul ...\n",
      "2024-09-25 13:03:06,323 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.21/attn/k_proj/MatMul ...\n",
      "2024-09-25 13:03:06,323 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.21/attn/v_proj/MatMul ...\n",
      "2024-09-25 13:03:06,336 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.21/attn/v_proj/MatMul ...\n",
      "2024-09-25 13:03:06,336 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.21/attn/o_proj/MatMul ...\n",
      "2024-09-25 13:03:06,349 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.21/attn/o_proj/MatMul ...\n",
      "2024-09-25 13:03:06,349 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.21/mlp/gate_proj/MatMul ...\n",
      "2024-09-25 13:03:06,385 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.21/mlp/gate_proj/MatMul ...\n",
      "2024-09-25 13:03:06,386 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.21/mlp/up_proj/MatMul ...\n",
      "2024-09-25 13:03:06,421 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.21/mlp/up_proj/MatMul ...\n",
      "2024-09-25 13:03:06,421 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.21/mlp/down_proj/MatMul ...\n",
      "2024-09-25 13:03:06,458 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.21/mlp/down_proj/MatMul ...\n",
      "2024-09-25 13:03:06,459 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.22/attn/q_proj/MatMul ...\n",
      "2024-09-25 13:03:06,473 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.22/attn/q_proj/MatMul ...\n",
      "2024-09-25 13:03:06,473 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.22/attn/k_proj/MatMul ...\n",
      "2024-09-25 13:03:06,486 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.22/attn/k_proj/MatMul ...\n",
      "2024-09-25 13:03:06,486 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.22/attn/v_proj/MatMul ...\n",
      "2024-09-25 13:03:06,500 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.22/attn/v_proj/MatMul ...\n",
      "2024-09-25 13:03:06,500 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.22/attn/o_proj/MatMul ...\n",
      "2024-09-25 13:03:06,515 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.22/attn/o_proj/MatMul ...\n",
      "2024-09-25 13:03:06,515 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.22/mlp/gate_proj/MatMul ...\n",
      "2024-09-25 13:03:06,552 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.22/mlp/gate_proj/MatMul ...\n",
      "2024-09-25 13:03:06,552 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.22/mlp/up_proj/MatMul ...\n",
      "2024-09-25 13:03:06,586 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.22/mlp/up_proj/MatMul ...\n",
      "2024-09-25 13:03:06,586 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.22/mlp/down_proj/MatMul ...\n",
      "2024-09-25 13:03:06,621 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.22/mlp/down_proj/MatMul ...\n",
      "2024-09-25 13:03:06,622 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.23/attn/q_proj/MatMul ...\n",
      "2024-09-25 13:03:06,636 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.23/attn/q_proj/MatMul ...\n",
      "2024-09-25 13:03:06,636 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.23/attn/k_proj/MatMul ...\n",
      "2024-09-25 13:03:06,650 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.23/attn/k_proj/MatMul ...\n",
      "2024-09-25 13:03:06,650 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.23/attn/v_proj/MatMul ...\n",
      "2024-09-25 13:03:06,657 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.23/attn/v_proj/MatMul ...\n",
      "2024-09-25 13:03:06,657 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.23/attn/o_proj/MatMul ...\n",
      "2024-09-25 13:03:06,677 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.23/attn/o_proj/MatMul ...\n",
      "2024-09-25 13:03:06,677 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.23/mlp/gate_proj/MatMul ...\n",
      "2024-09-25 13:03:06,713 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.23/mlp/gate_proj/MatMul ...\n",
      "2024-09-25 13:03:06,713 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.23/mlp/up_proj/MatMul ...\n",
      "2024-09-25 13:03:06,744 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.23/mlp/up_proj/MatMul ...\n",
      "2024-09-25 13:03:06,744 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.23/mlp/down_proj/MatMul ...\n",
      "2024-09-25 13:03:06,781 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.23/mlp/down_proj/MatMul ...\n",
      "2024-09-25 13:03:06,781 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.24/attn/q_proj/MatMul ...\n",
      "2024-09-25 13:03:06,791 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.24/attn/q_proj/MatMul ...\n",
      "2024-09-25 13:03:06,791 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.24/attn/k_proj/MatMul ...\n",
      "2024-09-25 13:03:06,806 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.24/attn/k_proj/MatMul ...\n",
      "2024-09-25 13:03:06,806 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.24/attn/v_proj/MatMul ...\n",
      "2024-09-25 13:03:06,821 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.24/attn/v_proj/MatMul ...\n",
      "2024-09-25 13:03:06,822 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.24/attn/o_proj/MatMul ...\n",
      "2024-09-25 13:03:06,831 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.24/attn/o_proj/MatMul ...\n",
      "2024-09-25 13:03:06,831 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.24/mlp/gate_proj/MatMul ...\n",
      "2024-09-25 13:03:06,872 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.24/mlp/gate_proj/MatMul ...\n",
      "2024-09-25 13:03:06,872 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.24/mlp/up_proj/MatMul ...\n",
      "2024-09-25 13:03:06,903 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.24/mlp/up_proj/MatMul ...\n",
      "2024-09-25 13:03:06,903 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.24/mlp/down_proj/MatMul ...\n",
      "2024-09-25 13:03:06,933 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.24/mlp/down_proj/MatMul ...\n",
      "2024-09-25 13:03:06,933 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.25/attn/q_proj/MatMul ...\n",
      "2024-09-25 13:03:06,953 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.25/attn/q_proj/MatMul ...\n",
      "2024-09-25 13:03:06,953 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.25/attn/k_proj/MatMul ...\n",
      "2024-09-25 13:03:06,964 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.25/attn/k_proj/MatMul ...\n",
      "2024-09-25 13:03:06,964 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.25/attn/v_proj/MatMul ...\n",
      "2024-09-25 13:03:06,980 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.25/attn/v_proj/MatMul ...\n",
      "2024-09-25 13:03:06,980 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.25/attn/o_proj/MatMul ...\n",
      "2024-09-25 13:03:06,994 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.25/attn/o_proj/MatMul ...\n",
      "2024-09-25 13:03:06,994 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.25/mlp/gate_proj/MatMul ...\n",
      "2024-09-25 13:03:07,027 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.25/mlp/gate_proj/MatMul ...\n",
      "2024-09-25 13:03:07,027 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.25/mlp/up_proj/MatMul ...\n",
      "2024-09-25 13:03:07,059 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.25/mlp/up_proj/MatMul ...\n",
      "2024-09-25 13:03:07,066 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.25/mlp/down_proj/MatMul ...\n",
      "2024-09-25 13:03:07,097 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.25/mlp/down_proj/MatMul ...\n",
      "2024-09-25 13:03:07,097 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.26/attn/q_proj/MatMul ...\n",
      "2024-09-25 13:03:07,107 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.26/attn/q_proj/MatMul ...\n",
      "2024-09-25 13:03:07,107 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.26/attn/k_proj/MatMul ...\n",
      "2024-09-25 13:03:07,127 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.26/attn/k_proj/MatMul ...\n",
      "2024-09-25 13:03:07,127 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.26/attn/v_proj/MatMul ...\n",
      "2024-09-25 13:03:07,137 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.26/attn/v_proj/MatMul ...\n",
      "2024-09-25 13:03:07,137 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.26/attn/o_proj/MatMul ...\n",
      "2024-09-25 13:03:07,150 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.26/attn/o_proj/MatMul ...\n",
      "2024-09-25 13:03:07,150 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.26/mlp/gate_proj/MatMul ...\n",
      "2024-09-25 13:03:07,188 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.26/mlp/gate_proj/MatMul ...\n",
      "2024-09-25 13:03:07,188 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.26/mlp/up_proj/MatMul ...\n",
      "2024-09-25 13:03:07,219 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.26/mlp/up_proj/MatMul ...\n",
      "2024-09-25 13:03:07,229 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.26/mlp/down_proj/MatMul ...\n",
      "2024-09-25 13:03:07,261 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.26/mlp/down_proj/MatMul ...\n",
      "2024-09-25 13:03:07,261 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.27/attn/q_proj/MatMul ...\n",
      "2024-09-25 13:03:07,276 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.27/attn/q_proj/MatMul ...\n",
      "2024-09-25 13:03:07,279 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.27/attn/k_proj/MatMul ...\n",
      "2024-09-25 13:03:07,291 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.27/attn/k_proj/MatMul ...\n",
      "2024-09-25 13:03:07,291 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.27/attn/v_proj/MatMul ...\n",
      "2024-09-25 13:03:07,306 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.27/attn/v_proj/MatMul ...\n",
      "2024-09-25 13:03:07,307 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.27/attn/o_proj/MatMul ...\n",
      "2024-09-25 13:03:07,310 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.27/attn/o_proj/MatMul ...\n",
      "2024-09-25 13:03:07,310 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.27/mlp/gate_proj/MatMul ...\n",
      "2024-09-25 13:03:07,357 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.27/mlp/gate_proj/MatMul ...\n",
      "2024-09-25 13:03:07,358 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.27/mlp/up_proj/MatMul ...\n",
      "2024-09-25 13:03:07,392 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.27/mlp/up_proj/MatMul ...\n",
      "2024-09-25 13:03:07,392 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.27/mlp/down_proj/MatMul ...\n",
      "2024-09-25 13:03:07,422 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.27/mlp/down_proj/MatMul ...\n",
      "2024-09-25 13:03:07,422 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.28/attn/q_proj/MatMul ...\n",
      "2024-09-25 13:03:07,433 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.28/attn/q_proj/MatMul ...\n",
      "2024-09-25 13:03:07,442 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.28/attn/k_proj/MatMul ...\n",
      "2024-09-25 13:03:07,452 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.28/attn/k_proj/MatMul ...\n",
      "2024-09-25 13:03:07,452 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.28/attn/v_proj/MatMul ...\n",
      "2024-09-25 13:03:07,465 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.28/attn/v_proj/MatMul ...\n",
      "2024-09-25 13:03:07,465 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.28/attn/o_proj/MatMul ...\n",
      "2024-09-25 13:03:07,481 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.28/attn/o_proj/MatMul ...\n",
      "2024-09-25 13:03:07,481 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.28/mlp/gate_proj/MatMul ...\n",
      "2024-09-25 13:03:07,516 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.28/mlp/gate_proj/MatMul ...\n",
      "2024-09-25 13:03:07,516 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.28/mlp/up_proj/MatMul ...\n",
      "2024-09-25 13:03:07,547 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.28/mlp/up_proj/MatMul ...\n",
      "2024-09-25 13:03:07,547 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.28/mlp/down_proj/MatMul ...\n",
      "2024-09-25 13:03:07,588 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.28/mlp/down_proj/MatMul ...\n",
      "2024-09-25 13:03:07,588 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.29/attn/q_proj/MatMul ...\n",
      "2024-09-25 13:03:07,603 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.29/attn/q_proj/MatMul ...\n",
      "2024-09-25 13:03:07,603 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.29/attn/k_proj/MatMul ...\n",
      "2024-09-25 13:03:07,607 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.29/attn/k_proj/MatMul ...\n",
      "2024-09-25 13:03:07,607 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.29/attn/v_proj/MatMul ...\n",
      "2024-09-25 13:03:07,628 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.29/attn/v_proj/MatMul ...\n",
      "2024-09-25 13:03:07,628 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.29/attn/o_proj/MatMul ...\n",
      "2024-09-25 13:03:07,638 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.29/attn/o_proj/MatMul ...\n",
      "2024-09-25 13:03:07,638 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.29/mlp/gate_proj/MatMul ...\n",
      "2024-09-25 13:03:07,678 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.29/mlp/gate_proj/MatMul ...\n",
      "2024-09-25 13:03:07,678 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.29/mlp/up_proj/MatMul ...\n",
      "2024-09-25 13:03:07,714 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.29/mlp/up_proj/MatMul ...\n",
      "2024-09-25 13:03:07,714 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.29/mlp/down_proj/MatMul ...\n",
      "2024-09-25 13:03:07,749 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.29/mlp/down_proj/MatMul ...\n",
      "2024-09-25 13:03:07,750 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.30/attn/q_proj/MatMul ...\n",
      "2024-09-25 13:03:07,764 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.30/attn/q_proj/MatMul ...\n",
      "2024-09-25 13:03:07,764 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.30/attn/k_proj/MatMul ...\n",
      "2024-09-25 13:03:07,777 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.30/attn/k_proj/MatMul ...\n",
      "2024-09-25 13:03:07,777 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.30/attn/v_proj/MatMul ...\n",
      "2024-09-25 13:03:07,791 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.30/attn/v_proj/MatMul ...\n",
      "2024-09-25 13:03:07,792 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.30/attn/o_proj/MatMul ...\n",
      "2024-09-25 13:03:07,801 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.30/attn/o_proj/MatMul ...\n",
      "2024-09-25 13:03:07,801 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.30/mlp/gate_proj/MatMul ...\n",
      "2024-09-25 13:03:07,840 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.30/mlp/gate_proj/MatMul ...\n",
      "2024-09-25 13:03:07,840 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.30/mlp/up_proj/MatMul ...\n",
      "2024-09-25 13:03:07,873 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.30/mlp/up_proj/MatMul ...\n",
      "2024-09-25 13:03:07,873 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.30/mlp/down_proj/MatMul ...\n",
      "2024-09-25 13:03:07,904 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.30/mlp/down_proj/MatMul ...\n",
      "2024-09-25 13:03:07,904 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.31/attn/q_proj/MatMul ...\n",
      "2024-09-25 13:03:07,924 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.31/attn/q_proj/MatMul ...\n",
      "2024-09-25 13:03:07,924 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.31/attn/k_proj/MatMul ...\n",
      "2024-09-25 13:03:07,941 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.31/attn/k_proj/MatMul ...\n",
      "2024-09-25 13:03:07,941 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.31/attn/v_proj/MatMul ...\n",
      "2024-09-25 13:03:07,954 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.31/attn/v_proj/MatMul ...\n",
      "2024-09-25 13:03:07,954 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.31/attn/o_proj/MatMul ...\n",
      "2024-09-25 13:03:07,965 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.31/attn/o_proj/MatMul ...\n",
      "2024-09-25 13:03:07,965 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.31/mlp/gate_proj/MatMul ...\n",
      "2024-09-25 13:03:08,004 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.31/mlp/gate_proj/MatMul ...\n",
      "2024-09-25 13:03:08,006 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.31/mlp/up_proj/MatMul ...\n",
      "2024-09-25 13:03:08,041 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.31/mlp/up_proj/MatMul ...\n",
      "2024-09-25 13:03:08,042 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /model/layers.31/mlp/down_proj/MatMul ...\n",
      "2024-09-25 13:03:08,076 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /model/layers.31/mlp/down_proj/MatMul ...\n",
      "2024-09-25 13:03:08,076 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - start to quantize /lm_head/MatMul ...\n",
      "2024-09-25 13:03:08,197 onnxruntime.quantization.matmul_4bits_quantizer [INFO] - complete quantization of /lm_head/MatMul ...\n",
      "C:\\Users\\NVIDIA\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:924: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\NVIDIA\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:786: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "!python -m onnxruntime_genai.models.builder -m microsoft/Phi-3.5-mini-instruct -e dml -p int4 -o phi-3-dml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3626322f-632a-49b1-84c1-8a6b06a31b74",
   "metadata": {},
   "source": [
    "After this command, we've generated a directory at `phi-3-dml` that contains a couple important things:\n",
    "  - The core ONNX model used for each inference pass\n",
    "  - The appropriate tokenizer for the model\n",
    "  - A ORT-GenAI config file. The specification is described [here](https://onnxruntime.ai/docs/genai/reference/config), but the model builder takes care of populating this entirely for us."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1696f6-f937-492f-aa04-737da9b5b7d6",
   "metadata": {},
   "source": [
    "## Inferencing the model using ORT-GenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcfdbd6-2e14-41e6-a240-d803f70890a5",
   "metadata": {},
   "source": [
    "The below cell will run a simple inference on the model we generated above on your local DirectML-supported device. If you look in the config file we generated at `phi-3-dml/genai_config.json`, you can find the `dml` Execution Provider in `model.decoder.session_options.provider_options`. This Execution Provider will be used by ORT-GenAI under the hood when we instantiate a `Model` using this config file.\n",
    "\n",
    "For the prompt construction, we follow the prompt format provided in the [Phi-3 model card.](https://huggingface.co/microsoft/Phi-3.5-mini-instruct#input-formats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9434f0a-336d-4c91-87f7-a3290f8c9fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " You are a helpful cooking assistant.\n",
      " \n",
      " Can you tell me a recipe that uses strawberries, milk, and whipped cream?\n",
      " \n",
      " Certainly! Here's a simple and delightful recipe for a Strawberry Milkshake with Whipped Cream Topping that you can enjoy:\n",
      "\n",
      "**Strawberry Milkshake with Whipped Cream Topping**\n",
      "\n",
      "**Ingredients:**\n",
      "\n",
      "* Fresh strawberries (about 1 cup, hulled and sliced)\n",
      "* 2 cups of vanilla ice cream (or any flavor of your choice)\n",
      "* 1/2 cup of milk (or more, to desired consistency)\n",
      "* Whipped cream (for topping)\n",
      "* Optional: A drizzle of honey or maple syrup for added sweetness\n",
      "* Fresh mint leaves (for garnish)\n",
      "\n",
      "**Instructions:**\n",
      "\n",
      "1. **Prepare the Strawberries:**\n",
      "   - Wash and hull about 1 cup of fresh strawberries. Slice them into halves or quarters, depending on your preference.\n",
      "\n",
      "2. **Make the Milkshake:**\n",
      "   - Place the sliced strawberries, vanilla ice cream, and milk into a blender or food processor.\n",
      "   - Blend until the mixture is smooth and creamy. You may add more milk if the consistency is too thick.\n",
      "   - Taste and adjust the sweetness if necessary by adding a drizzle of honey or maple syrup.\n",
      "\n",
      "3. **Chill the Milkshake:**\n",
      "   - Put the milkshake in the refriderator for a few minutes to chill, or serve immediately if you prefer it warm.\n",
      "\n",
      "4. **Whip the Cream:**\n",
      "   - In a separate bowl, whip the cold heavy cream with a mixer or hand whisk until it forms stiff peaks. This usually takes about 3-5 minutes.\n",
      "\n",
      "5. **Serve:**\n",
      "   - Fill a tall glass with ice cubes.\n",
      "   - Pour the chilled strawberry milkshake over the ice cubes.\n",
      "   - Carefully spoon a generous amount of whipped cream on top of the milkshake.\n",
      "   - If desired, garnish with a few mint leaves and a few extra strawberry slices.\n",
      "\n",
      "6. **Enjoy:**\n",
      "   - Take a moment to savor the combination of the sweet, creamy milkshake and the light, airy whipped cream.\n",
      "\n",
      "**Tips:**\n",
      "\n",
      "* For an extra touch of freshness, consider adding a few fresh strawberries on top of the whipped cream.\n",
      "* You can also blend the strawberries with the ice cream for a more integrated flavor.\n",
      "* Adjust the amount of milk and sweetener to your taste.\n",
      "* For a healthier option, consider using Greek yogurt instead of ice cream and almond or soy milk instead of dairy milk.\n",
      "* If you don't have a blender, you can muddle the strawberries by hand and mix them with the ice cream and milk in a bowl, then serve with a dollop of whipped cream on top.\n",
      "\n",
      "This recipe is perfect for a quick and delicious dessert or a refreshing treat on a hot day. Enjoy!\n",
      "\n",
      "Remember, the key to a great milkshake is the quality of the ingredients, so using fresh strawberries and good-quality ice cream will make a significant difference in the taste.\n",
      "\n",
      "-----\n",
      "\n",
      "This recipe is simple and doesn't require complex techniques or ingredients, making it ideal for both beginners and experienced home cooks. The whipped cream adds a light, airy contrast to the rich, sweet milkshake, creating a delightful sensory experience with each spoonful. Enjoy your homemade Strawberry Milkshake with Whipped Cream!\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime_genai as ort_genai\n",
    "\n",
    "model_path = \"phi-3-dml\"\n",
    "\n",
    "model = ort_genai.Model(model_path)\n",
    "tokenizer = ort_genai.Tokenizer(model)\n",
    "params = ort_genai.GeneratorParams(model)\n",
    "\n",
    "search_options = {\n",
    "    \"max_length\": 1000\n",
    "}\n",
    "\n",
    "params.set_search_options(**search_options)\n",
    "\n",
    "prompt = \"<|system|> \\n \\\n",
    "You are a helpful cooking assistant.<|end|> \\n \\\n",
    "<|user|> \\n \\\n",
    "Can you tell me a recipe that uses strawberries, milk, and whipped cream?<|end|> \\n \\\n",
    "<|assistant|> \\n\"\n",
    "\n",
    "params.input_ids = tokenizer.encode(prompt)\n",
    "\n",
    "\n",
    "output_ids = model.generate(params)\n",
    "\n",
    "print(tokenizer.decode(output_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c89d11-3eb2-4d0f-853d-27498e88e0e9",
   "metadata": {},
   "source": [
    "## Wrapping Up\n",
    "\n",
    "The installation, model building process, and inference loop execution of ORT-GenAI are simple and streamlined, and we've taken the simplest path to demonstrate the above using DirectML to execute on your local GPU / DML-supported device. ORT-GenAI offers a lot of flexibility that we haven't demonstrated, including:\n",
    "  - Vocabulary masking, which will restrict the tokens that the LLM can produce\n",
    "  - Various search algorithms, which can produce multiple candidates for output sequences\n",
    "  - Streaming output tokens as they're generated\n",
    "  - & Many more!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec59098-3f92-442e-ae4c-fabb6fa8f252",
   "metadata": {},
   "source": [
    "For additional resources to take you further into ORT-GenAI, see:\n",
    "\n",
    "  - [Deep Dive into ONNXRuntime-GenAI](https://www.youtube.com/watch?v=S_qufVKPwMM)\n",
    "  - [Microsoft's Model Builder documentation](https://github.com/microsoft/onnxruntime-genai/blob/main/src/python/py/models/README.md)\n",
    "  - [Python sample for streaming tokens](https://github.com/microsoft/onnxruntime-genai/blob/main/README.md#sample-code-for-phi-3-in-python)\n",
    "  - [DirectML Introduction](https://learn.microsoft.com/en-us/windows/ai/directml/dml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03a85ca-fd31-4e2a-9e91-eace4f22298a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
